{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <div>\n",
    "        <h2>Actividad WhitePapers</h2>\n",
    "        <h2>Miguel López</h2>\n",
    "        <h2>ID 1001014378</h2>\n",
    "    </div>\n",
    "    <img src=\"https://yt3.ggpht.com/-10IUL9wra6k/AAAAAAAAAAI/AAAAAAAAAAA/UOBLu1uYOOE/s900-c-k-no/photo.jpg\" alt=\"Image description\" width=\"200\" style=\"margin-left: 20px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una pequeña introducción a los transformers. LoRa, QLoRa y sus efectos en el fine-tuning de Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introducción\n",
    "Gracias al gran éxito de los Transformers, en especial de chatGPT por OpenAI y el gran interés por la industria en adoptar estas nuevas tecnologías. Ha generado la necesidad de modelos más exactos para diferentes casos de uso a nivel profesional. Para ello tradicionalmente se realizaba un reentrenamiento supervisado (con datos etiquetados) completo de los modelos. El cual podía dar a luz problemas como la pérdida de memoria del modelo para otras tareas [1] o principalmente el gran gasto computacional que implica entrenar un modelo por ejemplo GPT 3 con 175 billones de parámetros [2]. Además a diferencia de algunas técnicas de Deep learning no se puede aplicar simplemente un entrenamiento a las capas superficiales del modelo ya que por la naturaleza de los Transformers, reentrenar su ventana de atención, en otras palabras lo que le permite a estos modelos tener una coherencia de ideas con un texto dado [3]. Para dar solución a todo esto en 2021 se presentó LoRa o Low-Rank Adaptation of Large Language Models [2], donde a través de una adición de una pequeña matriz en capas especificas del modelo se podría reentrenar a un costo computacional menor, sin perder la mayoría de sus cualidades generales como Large Lenaguage Models (LLM) y con la capacidad de respuestas más precisas dado casos establecidos.\n",
    "\n",
    "\n",
    "Pero pese a esto, en términos de optimización de memoria se puede combinar con técnicas como cuantización para bajar el grado de precisión de los pesos flotantes de los modelos de 32 bits a 16, 8 o 4 bits y luego realizando un fine-tuning con LoRa, conocido como QLoRA [4]. Perdiendo un poco de precisión en el momento de sus respuestas, pero dejando las puertas abiertas a aplicaciones en por ejemplo teléfonos celulares o sistemas embebidos para su uso como productos funcionales.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Funcionamiento de Transformers y sus implicaciones.\n",
    "\n",
    "Los transfomers parten de la idea del como mantener la atención de un modelo a lo largo de sus neuronas sin perder ese foco de atención entre ellas. Usando la arquitectura de la forma:\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/transformers.png\" alt=\"Fig 1. Arquitectura de Transformers [3]\">\n",
    "    <p><em>Fig 1. Arquitectura de Transformers [3]</em></p>\n",
    "</div>\n",
    "\n",
    "Donde inicialmente nos interesa la etapa de encoding, donde las palabras son traducidas a un espacio vectorial dependiendo de su similitud semántica. Como se puede observar a continuación:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/3d_wordvec_3.png\" alt=\"Fig 2. Demostración de un embedings en un espacio 3D [5]\" width=\"50%\">\n",
    "    <p><em>Fig 2. Demostración de un embedings en un espacio 3D [5]</em></p>\n",
    "</div>\n",
    "\n",
    "De esta manera pudiendo operar matricialmente las palabras (en forma de vectores) con la arquitectura mostrada en la Fig 1.\n",
    "\n",
    "Anteriormente usando redes neuronales recurrentes (RNN) las cuales después de cada capa iban perdiendo la atención al texto ingresado debido a su naturaleza secuencial, donde en el entrenamiento se puede dar el fenómeno del desvanecimiento de gradientes, el cual implica que los gradientes a medida que pasan por la backpropagation se vuelven 0 o un valor cercano a este impidiendo al modelo aprender nueva información [3] además que la complejidad para volver a esa información requiere un tamaño menor $O(1)$ de Transformers comparado con el $O(n)$ de RNN [3]. Esto se soluciona mediante capas de atención las cuales son dadas por una serie de matrices a las cuales se les aplica una función softmax de la forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que la función softmax  es [6]:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j} \\exp(x_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generación de embedings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "\n",
    "text = [\"El gato es azul\", \"El perro es verde\", \"El cielo es azul\", \"El pasto es verde\"]\n",
    "\n",
    "\n",
    "def encode_text(text, tokenizer):\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512 \n",
    "    )\n",
    "    return tokenized_inputs[\"input_ids\"][0]\n",
    "\n",
    "inputs = [encode_text(t, tokenizer).numpy() for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  101,  3449, 11721,  3406,  9686, 17207,  5313,   102]),\n",
       " array([  101,  3449,  2566,  3217,  9686, 16184,   102,     0]),\n",
       " array([  101,  3449, 25022, 18349,  9686, 17207,  5313,   102]),\n",
       " array([  101,  3449,  2627,  2080,  9686, 16184,   102,     0])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Encuentra la longitud máxima de las secuencias en 'inputs'\n",
    "max_length = max(len(seq) for seq in inputs)\n",
    "\n",
    "# Agrega padding de ceros a cada secuencia para que todas tengan la misma longitud\n",
    "padded_inputs = [np.pad(seq, (0, max_length - len(seq)), 'constant') for seq in inputs]\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa de atención\n",
      " [[0.2517603  0.24117014 0.26620964 0.24085991]\n",
      " [0.25137197 0.24425888 0.26028139 0.24408776]\n",
      " [0.25219952 0.23427593 0.27991041 0.23361414]\n",
      " [0.25133767 0.24455666 0.25970371 0.24440196]]\n",
      "\n",
      "Salida de la atención\n",
      " [[0.30272187 0.37315351 0.24318054 0.17621523]\n",
      " [0.30153986 0.3715292  0.24174308 0.17500056]\n",
      " [0.30541302 0.37684938 0.24646852 0.17899182]\n",
      " [0.30142499 0.37137136 0.24160326 0.17488242]]\n"
     ]
    }
   ],
   "source": [
    "# Iniciando con numpy aunque este proceso se hace por medio de tensores\n",
    "L, d_v, d_k = len(padded_inputs[0]), 4, 4\n",
    "\n",
    "\n",
    "\n",
    "# Generamos valores aleatorios para Q, K, y V\n",
    "np.random.seed(23)  # Semilla para reproducibilidad\n",
    "\n",
    "Q = np.matmul(padded_inputs, np.random.rand(L, d_k) * 1e-5)\n",
    "V = np.matmul(padded_inputs, np.random.rand(L, d_v) * 1e-5)\n",
    "K = np.matmul(padded_inputs, np.random.rand(L, d_k) * 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Calculamos la atención\n",
    "# QK^T\n",
    "QK_T = np.matmul(Q, K.T)\n",
    "\n",
    "# Dividimos por la raíz cuadrada de d_k\n",
    "QK_T_scaled = QK_T / np.sqrt(d_k)\n",
    "\n",
    "# Aplicamos la función softmax\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "attention_weights = softmax(QK_T_scaled)\n",
    "\n",
    "# Calculamos la salida de la atención\n",
    "output = np.matmul(attention_weights, V)\n",
    "\n",
    "\n",
    "print(\"Capa de atención\\n\", attention_weights)\n",
    "\n",
    "print(\"\\nSalida de la atención\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tal vez agregar lo de las cabezas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo usamos 4 dimensiones, aunque en realidad la entrada de las matrices de atención son mayores, por ejemplo si quisiéramos reentrenar un modelo como GPT 3 el cual cuenta con 16.385 dimensiones [7] y 175 billones de parámetros [2]. Se incrementa el gasto computacional para cada iteración. Por ende el uso de menos parámetros especializados para ajustar un modelo es necesario y una de estas soluciones es LoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.LoRa [2]\n",
    "Este permite un ajuste eficiente, mediante la descomposición de bajo rango de las matrices de atención. Permitiendo un mejor ajuste con pocos parámetros. Esto quiere decir que si partimos de el cómo funciona una LLM de forma general tenemos que siendo $W_0$ las dimensiones de los pesos iniciales de un modelo entrenado su ajuste esta dado por un $\\Delta W$ para que este de una salida de predicción $h$ de la forma:\n",
    "$$\n",
    "h = W_0 x + \\Delta W x\n",
    "$$\n",
    "\n",
    "Supongamos así que existe una descomposición para $\\Delta W$ donde $B \\in \\mathbb{R}^{d \\times r}$ y $A \\in \\mathbb{R}^{r \\times k}$, donde r es el rank y d las dimensiones de la matriz del modelo. Así si $\\nabla(W) x = BA x$ tenemos que:\n",
    "$$\n",
    "h = W_0 x + BA x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De una manera gráfica se puede expresar mediante la comparación de:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/reaparam.png\" alt=\"Fig 3. Reparametrización en la que se basa LoRa [2]\" width=\"50%\">\n",
    "    <p><em>Fig 3. Reparametrización en la que se basa LoRa [2]</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así los pesos de $A$ y $B$ son cambiados en el fine-tuning del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de h:\n",
      "[0.63938988 0.41702811 0.6272325  0.39643064]\n",
      "\n",
      "Ajuste de pesos normal:\n",
      "[[0.63938988 0.41702811 0.6272325  0.39643064]]\n",
      "\n",
      "Con una matriz delta_W:\n",
      "[[-0.06088618 -0.49694735 -0.44204427 -0.4775679 ]\n",
      " [-0.03393741 -0.27699402 -0.24639153 -0.26619208]\n",
      " [-0.00223585 -0.01824883 -0.01623269 -0.01753718]\n",
      " [-0.07928901 -0.64714951 -0.575652   -0.62191263]]\n",
      "\n",
      "Tamaños de W y A*B iguales: True\n",
      "\n",
      "Ajuste de pesos con A y B:\n",
      "[[0.63938988 0.41702811 0.6272325  0.39643064]]\n",
      "\n",
      "Con A:\n",
      "[[0.56478545 0.2982329  0.18519292 0.78085279]\n",
      " [0.15521475 0.60111895 0.73804002 0.5663141 ]]\n",
      "\n",
      "Con B:\n",
      "[[-0.35074381 -0.49795333]\n",
      " [-0.19550147 -0.27755474]\n",
      " [-0.01287996 -0.01828577]\n",
      " [-0.45675601 -0.64845955]]\n"
     ]
    }
   ],
   "source": [
    "#De una manera práctica se tiene:\n",
    "\n",
    "#Supongamos un valor h\n",
    "\n",
    "h = np.random.rand(d_k)\n",
    "\n",
    "#Donde los pesos iniciales son aleatorios\n",
    "W = np.random.rand(d_k, d_v)\n",
    "\n",
    "#Establecemos un X de entrada \n",
    "\n",
    "X = np.random.rand(d_k, 1)\n",
    "\n",
    "def normal_tuning(W, X, h):\n",
    "    #Calculamos el producto punto de W y X\n",
    "    WX = np.dot(W, X)\n",
    "    #Generamos la matriz delta_W en ceros\n",
    "    delta_W = np.zeros_like(W)\n",
    "    #Ajustamos los pesos de delta_W a base de h\n",
    "    for i in range(W.shape[0]):\n",
    "        delta_W[i], _, _, _ = np.linalg.lstsq(X.T, h[i] - WX[i], rcond=None)\n",
    "    return delta_W\n",
    "\n",
    "delta_W = normal_tuning(W, X, h)\n",
    "\n",
    "#Solución de h\n",
    "normal_h = np.dot(W, X) + np.dot(delta_W, X)\n",
    "\n",
    "print(\"Valor de h:\")\n",
    "print(h)\n",
    "\n",
    "print(\"\\nAjuste de pesos normal:\")\n",
    "print(normal_h.T)\n",
    "\n",
    "print(\"\\nCon una matriz delta_W:\")\n",
    "print(delta_W)\n",
    "\n",
    "#De la misma manera podemos ajustar los pesos de A y B, en este caso para ser prácticos solo ajustaremos\n",
    "#Los pesos de B al ser todos 0\n",
    "\n",
    "#Creamos la clase de LoRa\n",
    "class LoRa_tuning:\n",
    "    def __init__(self, A, h, W, X,r):\n",
    "        #Inicializamos las variables\n",
    "        self.A = A\n",
    "        self.h = h\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "        self.B = np.zeros((W.shape[0], r))\n",
    "        self.d = W.shape[0]\n",
    "    def __call__(self):\n",
    "        #Calculamos los productos punto de A y X y W y X\n",
    "        Ax = np.dot(self.A, self.X)\n",
    "        Wx = np.dot(self.W, self.X)\n",
    "        #Ajustamos los pesos de B a base de h\n",
    "        for i in range(self.B.shape[0]):\n",
    "            self.B[i], _, _, _ = np.linalg.lstsq(Ax.T, h[i] - Wx[i], rcond=None)\n",
    "        return self.B\n",
    "    def solution(self):\n",
    "        # Calculamos la solución\n",
    "        return np.dot(self.W, self.X) + np.dot(np.dot(self.B, self.A), self.X)\n",
    "    def is_equal(self):\n",
    "        #Confirmamos que los tamaños de W y A*B sean iguales\n",
    "        return self.W.shape == np.dot(self.B, self.A).shape\n",
    "    def a_dot_b(self):\n",
    "        #Calculamos A*B\n",
    "        return np.dot(self.B, self.A)\n",
    "\n",
    "r = 2\n",
    "A = np.random.rand(r, d_k)\n",
    "\n",
    "\n",
    "LoRa = LoRa_tuning(A, h, W, X,r)\n",
    "B = LoRa()\n",
    "\n",
    "#Confirmamos que los tamaños de W y A*B sean iguales\n",
    "print(\"\\nTamaños de W y A*B iguales:\", LoRa.is_equal())\n",
    "\n",
    "print(\"\\nAjuste de pesos con A y B:\")\n",
    "print(LoRa.solution().T)\n",
    "\n",
    "print(\"\\nCon A:\")\n",
    "print(LoRa.A)\n",
    "\n",
    "print(\"\\nCon B:\")\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo el mismo resultado de ajuste, con menos parámetros que calcular, en nuestro ejemplo solo cambiando de pesos los 8 valores de la matriz $B$ a diferencia de los 16 del $\\Delta W$.Así reduciendo la cantidad de parámetros a utilizar 2/3 de la VRAM necesaria para el entrenamiento. Además de poder entrenar modelos más grandes como GPT-3 en menos GPU reduciendo así la latencia entre estas además del costo de computo y un aumento en la velocidad de entrenamiento en aproximadamente 25 % [2].\n",
    "\n",
    "\n",
    "Más sin embargo si se requieren hacer pruebas de concepto o encapsular modelos en microcontroladores o situaciones donde no se tenga tanta memoria para inferencia o entrenamiento, se requiere un paso extra tanto para su entrenamiento como su almacenamiento y una de estas soluciones es la cuantización. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. QLoRa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "En este método contamos de dos partes, la primera ya conocida de cómo se puede aplicar LoRa y el porqué es útil para realizar el fine-tuning de LLMs. Y ahora la cuantización. Esta técnica permite reducir los bits en los cuales se guardan los pesos de un modelo.\n",
    "\n",
    "Como mencionamos anteriormente en el caso de GPT 3 contiene 175 billones de parámetros [2], los cuales están almacenados en pesos de flotante a 32 bits, o en el caso de la mayoría de LLM actuales de 16 bits. Sin embargo, un modelo de 65 billones de parámetros de 780 GB a menos de 48 GB con una eficiencia similar al modelo entrenado completamente [4].\n",
    "\n",
    "Esto se logró mediante la suposición de que los pesos de los modelos siguen una distribución normal, de esta forma se pueden segmentar todas las secciones de la curva en sectores más grandes o pequeños que tendrán una nueva representación de los parámetros. Dicho proceso puede ser revertido a la hora de inferencia y además con el uso de un compaginador de memoria que puede ser trasladada a la CPU para evitar el sobre flujo en la GPU como se puede observar a continuación.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/quanti.png\" alt=\"Fig 4. Métodos de fine-tuning y QLoRa [4]\" width=\"60%\">\n",
    "    <p><em>Fig 4. Métodos de fine-tuning y QLoRa [4] </em></p>\n",
    "</div>\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Así, la cuantización se puede lograr por medio de bloques, en si la cuantización se logra por dos métodos 4 bit NormalFloat (NF4) y una doble cuantización. El final es “discretizar” la información como se puede observar a continuación"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/ex_cuanti.png\" alt=\"Fig 5. Ejemplo gráfico de como funciona la cuantización [8]\" width=\"60%\">\n",
    "    <p><em>Fig 5. Ejemplo gráfico de como funciona la cuantización [8]</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Donde los sw son los quantiles a discretizar y como se observa la mayoría de datos discretizados se encuentran cercanos a la media de la distribución"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**NF4**: en este método se usan quantiles donde se agrupan valores de la distribución, dichos quantiles pueden codificar los datos del grupo de números que estén dentro de su conjunto, la principal dificultad es que no se tienen en cuenta los valores extremos de los pesos que en algunos casos son las partes más importantes en los modelos [4]. Para mitigar esto se pueden normalizar los pesos del modelo o escalarlos para una distribución en la que pueda funcionar el NF4, dalos de -1 a 1, con una distribución estándar de $\\sigma$. Así se pueden obtener dos partes de una distribución normal, una positiva y negativa las cuales son representadas por un $2^{k-1}$ para la parte positiva y $2^{k-1}+1$. Para dicha estimación se usa la siguiente fórmula:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "q_i = \\frac{1}{2} \\left( Q_X\\left(\\frac{i}{2^k + 1}\\right) + Q_X\\left(\\frac{i+1}{2^k + 1}\\right) \\right)\n",
    "$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Doble cuantización**: Como su nombre lo indica, es una cuantización doble. Esto trae ventajas en términos de memoria, ya que se reduce la huella dependiendo del bloque de memoria a usar, en el caso del paper se usan 64 para cada peso dando como resultando una constante de cuantización de $32 bits /64 = 0.5 bits$, y obteniendo así un peso de 8 bits, el cual pasa por otro proceso de cuantización se tiene $8 bits / 64 + 32 bits / (64 * 256)$ (del paso anterior) $= 0.127$.\n",
    "\n",
    "Estos dos métodos combinados con LoRa dependiendo del caso obtiene resultandos muy similares a un entrenamiento normal.\n"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "#Para mostrar como se puede realizar este proceso se toma un modelo base",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viu-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
